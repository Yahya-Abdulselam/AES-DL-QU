{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils as gu\n",
    "data = gu.read_data(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_score(score, lower_bound, upper_bound):\n",
    "    range = upper_bound - lower_bound\n",
    "    return (score - lower_bound) / range # we divide the difference from the minimum value by the range to get a value between 0 and 1\n",
    "def unscale_score(scaled_score, lower_bound, upper_bound):\n",
    "    range=upper_bound-lower_bound\n",
    "    return scaled_score*range+lower_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dicts=[] # we will store the data for each prompt in a list of dictionaries, one dictionary \n",
    "#  one dictionary  will have prompt_id and a list of essays for that prompt with their features and scaled holistic scores\n",
    "for prompt_id in range(1,9):\n",
    "    essays_for_prompt=[]\n",
    "    for line_num in range(len(data['essay_ids'])):\n",
    "        \n",
    "        if data['prompt_ids'][line_num]==prompt_id:\n",
    "            range_score=gu.SCORE_RANGES[prompt_id]['holistic']\n",
    "            essays_for_prompt.append({'essay_id':data['essay_ids'][line_num],'holistic':scale_score(data['holistic'][line_num],range_score[0],range_score[1]),'features':data['features'][line_num]})\n",
    "\n",
    "    prompts_dicts.append({'prompt_id':prompt_id,'essays':essays_for_prompt})\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "num_hidden_layers = [1,2,4,8]\n",
    "num_hidden_units_per_layer = [8,16,32]\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "batch_size_initial=4\n",
    "batch_size = [8,16,32]\n",
    "# design the layers based on input\n",
    "def get_layers( input_size, layers_size,hidden_size, output_size):\n",
    "    layers = []\n",
    "    for i in range(layers_size):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "        else:\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(hidden_size, output_size))\n",
    "    return layers\n",
    "class HolisticFFN(nn.Module):\n",
    "    def __init__(self, input_size, layers_size,hidden_size, output_size):\n",
    "        super(HolisticFFN, self).__init__()\n",
    "        layers=get_layers(input_size, layers_size,hidden_size, output_size)\n",
    "        self.model=nn.Sequential(*layers) # put the layers in sequential order in the neurtal network model\n",
    "        \n",
    "     \n",
    "        self.apply(self.weights_HE_init)# He initialization for weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    def weights_HE_init(self,layer_in):\n",
    "     if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer_in.weight)\n",
    "        layer_in.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train_model(model, loss, optimizer, train_set, validation_set, device, range_score, batch_size=4, num_epochs=15):\n",
    "    best_qwk = 0\n",
    "    without_improvement = 0 # to decide when to early stop\n",
    "    best_model = model\n",
    "    model.to(device)# move the model to the gpu if available in the machine\n",
    "    train_set = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    validation_setLoader = DataLoader(validation_set)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs ):\n",
    "        model.train() \n",
    "        total_loss = 0\n",
    "        for features, scores in train_set:\n",
    "            features, scores = features.to(device), scores.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()# zero the gradients for the previous iteration\n",
    "            y_pred = model(features).view(-1)# view(-1) was used to flatten outputo ensure its same as true scores\n",
    "          \n",
    "            l = loss(y_pred, scores)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += l.item()\n",
    "        print(f\"Epoch {epoch + 1} loss: {total_loss/len(train_set)}\")\n",
    "        if validation_set is None: # in some cases we may not have validation set like when training the deployed model\n",
    "            continue\n",
    "        model.eval()# set the model to evaluation mode\n",
    "        predicted_scores, validation_scores = [], [] # to store the predicted and true scores\n",
    "        total_loss = 0\n",
    "        with torch.no_grad(): # we don't need to calculate gradients for validation set\n",
    "            \n",
    "            for features, score in validation_setLoader:\n",
    "                features, score = features.to(device), score.to(device)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "                y_pred = model(features).view(-1)\n",
    "                l=loss(y_pred, score)\n",
    "                total_loss += l.item()\n",
    "                \n",
    "                predicted_scores.append(y_pred.item())\n",
    "                validation_scores.append(score.item())\n",
    "               \n",
    "\n",
    "          # qwk only works with integers\n",
    "            validation_scores = [int(round(unscale_score(score, range_score[0], range_score[1]))) for score in validation_scores] #\n",
    "            predicted_scores = [int(round(unscale_score(pred, range_score[0], range_score[1]))) for pred in predicted_scores]\n",
    "            print('validation_scores:',validation_scores)\n",
    "            print('predicted_scores:',predicted_scores)\n",
    "            validation_qwk = gu.quadratic_weighted_kappa(validation_scores, predicted_scores)\n",
    "            \n",
    "            if validation_qwk > best_qwk:\n",
    "                best_qwk = validation_qwk\n",
    "                best_model = model\n",
    "                without_improvement = 0\n",
    "            else:\n",
    "                without_improvement += 1\n",
    "\n",
    "            if without_improvement == 4: # wait for 4 epochs without improvement before early stopping\n",
    "                print(f\"Early stopping on epoch {epoch + 1} qwk: {best_qwk}  loss: {total_loss/len(validation_setLoader)}\")\n",
    "                return best_qwk,best_model\n",
    "\n",
    "    return best_qwk,best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cross_validation( prompt_num,layer_size,hidden_size,lr, device,batch_size=4, num_epochs=15):\n",
    "\n",
    "    avg_QWKs = []\n",
    "    # each prompt will be used as the validation set once\n",
    "    for fold in range(8): \n",
    "        if prompt_num == fold+1: # we don't want to train on the validation set\n",
    "            continue\n",
    "        range_score=gu.SCORE_RANGES[fold+1]['holistic']\n",
    "        # normal lists are not compatible with pytorch, so we convert them to  numpy arrays and then to tensors\n",
    "        features = torch.tensor(np.array([essay['features'] for essay in prompts_dicts[fold]['essays']], dtype=np.float32)).to(device)\n",
    "        scores = torch.tensor(np.array([essay['holistic'] for essay in prompts_dicts[fold]['essays']], dtype=np.float32)).to(device)\n",
    "\n",
    "        validation_dataset=TensorDataset(features, scores)\n",
    "        train_features = []\n",
    "        train_scores = []\n",
    "        for prompt_index in range(8):\n",
    "          # we don't want to train on the validation set nor the test set\n",
    "          if prompt_index == fold:\n",
    "        \n",
    "              continue\n",
    "          if prompt_num == fold+1:\n",
    "            continue\n",
    "          train_features.extend([essay['features'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "          train_scores.extend([essay['holistic'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        model=HolisticFFN(86,layer_size,hidden_size,1)\n",
    "        model.to(device)\n",
    "        loss=nn.MSELoss()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr,betas=(0.9, 0.999),  weight_decay=0.1)\n",
    "       # normal lists are not compatible with pytorch, so we convert them to  numpy arrays and then to tensors\n",
    "        train_features = torch.tensor(np.array(train_features, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "        train_scores = torch.tensor(np.array(train_scores, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "   \n",
    "        train_dataset = TensorDataset(train_features, train_scores)\n",
    "       \n",
    "     \n",
    "        train_qwk,model=train_model(model, loss, optimizer,train_dataset, validation_dataset,device,range_score,batch_size=batch_size,num_epochs=num_epochs)\n",
    "        avg_QWKs.append(train_qwk )\n",
    "    final_avg_QWK = sum(avg_QWKs) / len(avg_QWKs)\n",
    "    print(f\"Average QWK over all folds: {final_avg_QWK}\")\n",
    "\n",
    "    return final_avg_QWK\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_size_tuning( prompt_num,best_params, batch_sizes, device):\n",
    "    best_batch_size = None\n",
    "    best_QWK = 0\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Testing batch size: {batch_size}\")\n",
    "        QWK = cross_validation(prompt_num,\n",
    "          best_params[0],\n",
    "           best_params[1],\n",
    "            best_params[2],\n",
    "          device,\n",
    "            batch_size,\n",
    "        )\n",
    "        if QWK > best_QWK:\n",
    "            best_QWK = QWK\n",
    "            best_batch_size = batch_size\n",
    "    print(f\"Best Batch Size: {best_batch_size}, QWK: {best_QWK}\")\n",
    "    return best_batch_size,best_QWK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get the training set based on the prompt number\n",
    "# all prompt will be taken for the set except the prompt number because its used for testing\n",
    "def get_trainSet(prompt_num,device):\n",
    "    train_features = []\n",
    "    train_scores = []\n",
    "    for prompt_index in range(8):\n",
    "        if prompt_num and prompt_index == prompt_num-1:\n",
    "            continue\n",
    "        train_features.extend([essay['features'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        train_scores.extend([essay['holistic'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "    train_features = torch.tensor(np.array(train_features, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    train_scores = torch.tensor(np.array(train_scores, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    train_dataset = TensorDataset(train_features, train_scores)\n",
    "    return train_dataset\n",
    "# we will get the validation set based on the prompt number\n",
    "def get_validationSet(prompt_num,device):\n",
    "    features = torch.tensor(np.array([essay['features'] for essay in prompts_dicts[prompt_num-1]['essays']], dtype=np.float32)).to(device)\n",
    "    scores = torch.tensor(np.array([essay['holistic'] for essay in prompts_dicts[prompt_num-1]['essays']], dtype=np.float32)).to(device)\n",
    "    validation_dataset = TensorDataset(features, scores)\n",
    "    return validation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(prompt_num, device):\n",
    "   \n",
    "    best_QWK = 0\n",
    "    best_params = None\n",
    "    \n",
    "  \n",
    "    for layers_size in num_hidden_layers:\n",
    "        for hidden_size in num_hidden_units_per_layer:\n",
    "            for lr in learning_rate:\n",
    "                print(f\"Prompt {prompt_num} - Testing: Layers={layers_size}, \"\n",
    "                      f\"Hidden size={hidden_size}, Learning rate={lr}\")\n",
    "                \n",
    "                QWK = cross_validation(prompt_num, layers_size, hidden_size, lr, device, 4)\n",
    "                \n",
    "                if QWK > best_QWK:\n",
    "                    best_QWK = QWK\n",
    "                    best_params = (layers_size, hidden_size, lr)\n",
    "                    print(f\"New best QWK: {best_QWK:.4f} with params: {best_params}\")\n",
    "    \n",
    "    # Tune batch size with best parameters\n",
    "    best_batch_size, batch_qwk= batch_size_tuning(prompt_num, best_params, batch_size, device)\n",
    "    if best_QWK > batch_qwk:\n",
    "        best_batch_size = 4\n",
    "    best_params = (*best_params, best_batch_size)\n",
    "    \n",
    "    return best_params, best_QWK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def train_and_save_models():\n",
    "   \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")# gpu if available\n",
    "    best_QWK_ALL = 0\n",
    "    results = {}\n",
    "\n",
    "\n",
    "    for prompt_num in range(1, 3):\n",
    "        print(f\"\\nTraining model for Prompt {prompt_num}\")\n",
    "        \n",
    "        best_params, best_QWK = find_best_hyperparameters(prompt_num, device)\n",
    "        layers_size, hidden_size, lr, batch_size = best_params\n",
    "        \n",
    "       ## train model based on best hyperparameters\n",
    "        model = HolisticFFN(86, layers_size, hidden_size, 1)\n",
    "        model.to(device)\n",
    "        loss = nn.MSELoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, \n",
    "                                    betas=(0.9, 0.999), weight_decay=0.1)\n",
    "        \n",
    "        train_set = get_trainSet(prompt_num, device)\n",
    "        val_set = get_validationSet(prompt_num, device)\n",
    "\n",
    "        qwk, trained_model = train_model(model, loss, optimizer, train_set, \n",
    "                                       val_set, device, gu.SCORE_RANGES[prompt_num]['holistic'], \n",
    "                                       batch_size, num_epochs=15)\n",
    "      \n",
    "        scrtipted_model = torch.jit.script(trained_model)\n",
    "        scrtipted_model.save(f\"model-A-{prompt_num}.pt\")\n",
    "        \n",
    "        \n",
    "        results[prompt_num] = {\n",
    "            'layers_size': layers_size,\n",
    "            'hidden_size': hidden_size,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'qwk': qwk\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nPrompt {prompt_num} Results:\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        print(f\"Final QWK: {qwk:.4f}\")\n",
    " \n",
    "        \n",
    "        if qwk > best_QWK_ALL:\n",
    "            best_QWK_ALL = qwk\n",
    "    \n",
    "    print(\"\\n Summary of Results:\")\n",
    "    for prompt_num, params in results.items():\n",
    "        print(f\"\\nPrompt {prompt_num}:\")\n",
    "        print(f\"Layers: {params['layers_size']}\")\n",
    "        print(f\"Hidden Size: {params['hidden_size']}\")\n",
    "        print(f\"Learning Rate: {params['learning_rate']}\")\n",
    "        print(f\"Batch Size: {params['batch_size']}\")\n",
    "        print(f\"QWK: {params['qwk']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Overall QWK: {best_QWK_ALL:.4f}\")\n",
    "    return best_QWK_ALL, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for Prompt 1\n",
      "Prompt 1 - Testing: Layers=1, Hidden size=8, Learning rate=0.001\n",
      "Epoch 1 loss: 0.033879262082579034\n",
      "validation_scores: [4, 1, 2, 4, 4, 4, 5, 2, 4, 4, 3, 3, 5, 3, 3, 3, 4, 3, 1, 3, 3, 3, 3, 4, 3, 4, 4, 2, 3, 3, 4, 3, 3, 4, 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 3, 5, 3, 4, 3, 4, 4, 4, 2, 3, 4, 2, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, 4, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 1, 4, 2, 4, 3, 4, 4, 4, 4, 4, 3, 5, 3, 3, 4, 4, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 4, 2, 1, 3, 3, 4, 3, 3, 3, 3, 5, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 2, 4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 4, 3, 2, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4, 2, 1, 3, 3, 4, 5, 3, 4, 4, 3, 4, 3, 3, 5, 4, 3, 3, 4, 4, 1, 4, 4, 3, 4, 2, 3, 3, 4, 2, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4, 3, 2, 4, 4, 4, 5, 4, 3, 3, 4, 3, 4, 3, 4, 3, 4, 4, 4, 3, 4, 4, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 4, 2, 4, 3, 3, 3, 4, 4, 2, 2, 4, 2, 1, 4, 4, 2, 2, 3, 4, 4, 3, 2, 3, 4, 4, 5, 3, 3, 1, 4, 2, 4, 4, 4, 4, 2, 4, 4, 3, 3, 4, 3, 4, 2, 4, 4, 3, 3, 5, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 5, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 3, 3, 5, 3, 4, 4, 6, 4, 4, 3, 3, 4, 3, 3, 3, 3, 4, 4, 3, 4, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 3, 4, 5, 3, 3, 2, 4, 4, 3, 3, 4, 2, 2, 3, 3, 3, 3, 3, 2, 4, 4, 4, 2, 4, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 2, 3, 1, 3, 3, 4, 5, 4, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 3, 4, 4, 3, 4, 3, 3, 3, 5, 2, 4, 4, 2, 2, 4, 4, 1, 4, 6, 2, 3, 3, 4, 3, 2, 2, 4, 3, 3, 3, 4, 2, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 2, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3, 4, 4, 4, 4, 3, 3, 2, 4, 2, 3, 3, 2, 3, 3, 4, 3, 4, 3, 4, 1, 4, 3, 3, 4, 2, 4, 2, 2, 4, 4, 3, 4, 2, 2, 4, 3, 3, 4, 3, 4, 4, 3, 5, 3, 3, 4, 3, 4, 3, 4, 2, 4, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 3, 3, 3, 3, 4, 1, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 3, 5, 4, 2, 4, 4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 2, 4, 2, 3, 3, 4, 4, 4, 4, 2, 3, 3, 4, 4, 3, 2, 3, 3, 4, 4, 4, 4, 3, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 5, 3, 4, 3, 5, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 2, 3, 2, 4, 3, 4, 4, 5, 4, 4, 4, 4, 4, 4, 3, 2, 4, 5, 3, 3, 4, 4, 2, 4, 3, 3, 2, 4, 4, 1, 3, 3, 3, 4, 3, 4, 4, 4, 3, 3, 3, 4, 4, 3, 2, 3, 4, 4, 4, 3, 4, 4, 3, 3, 3, 4, 4, 4, 2, 4, 3, 3, 3, 4, 2, 3, 5, 4, 3, 2, 3, 3, 4, 2, 4, 3, 3, 3, 4, 3, 4, 5, 5, 2, 3, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 4, 4, 3, 2, 4, 2, 4, 3, 3, 2, 4, 4, 4, 3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 2, 4, 4, 2, 4, 3, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 4, 4, 4, 3, 2, 4, 4, 3, 4, 5, 2, 4, 3, 4, 3, 3, 4, 3, 5, 3, 4, 4, 3, 4, 3, 3, 2, 4, 3, 4, 3, 3, 4, 3, 5, 3, 3, 3, 2, 3, 4, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 4, 3, 3, 3, 4, 2, 4, 4, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 5, 3, 4, 4, 4, 3, 4, 3, 3, 4, 4, 5, 3, 5, 4, 4, 3, 1, 3, 5, 3, 1, 3, 3, 3, 4, 5, 4, 3, 3, 3, 4, 3, 4, 4, 4, 3, 4, 3, 3, 4, 2, 4, 3, 4, 4, 5, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 3, 3, 4, 3, 5, 3, 3, 3, 3, 3, 4, 4, 3, 4, 3, 3, 5, 3, 4, 4, 2, 3, 4, 4, 3, 4, 3, 3, 4, 3, 5, 2, 4, 4, 4, 4, 2, 3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 3, 3, 4, 3, 4, 3, 2, 4, 4, 4, 4, 3, 3, 4, 4, 5, 4, 3, 3, 3, 4, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 2, 4, 3, 4, 2, 4, 3, 3, 2, 1, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 1, 4, 5, 4, 5, 3, 3, 3, 3, 5, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 5, 3, 4, 3, 3, 4, 2, 3, 3, 4, 3, 4, 3, 3, 4, 4, 5, 6, 4, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3, 2, 3, 4, 3, 4, 4, 4, 3, 3, 3, 3, 6, 4, 3, 4, 2, 3, 4, 2, 4, 4, 3, 3, 2, 3, 3, 4, 2, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 2, 2, 4, 4, 4, 2, 5, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, 5, 3, 3, 5, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 5, 4, 4, 3, 4, 3, 4, 4, 3, 4, 3, 3, 3, 3, 4, 2, 4, 3, 4, 4, 3, 3, 4, 4, 2, 4, 5, 4, 3, 3, 3, 3, 5, 4, 2, 2, 4, 4, 4, 4, 3, 6, 4, 5, 4, 2, 4, 3, 3, 3, 4, 4, 4, 4, 4, 2, 4, 3, 3, 3, 2, 3, 3, 3, 2, 4, 3, 3, 4, 2, 4, 2, 4, 3, 5, 4, 4, 1, 4, 3, 3, 4, 2, 3, 3, 4, 4, 3, 2, 4, 3, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3, 4, 2, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 3, 4, 3, 3, 4, 4, 4, 3, 4, 4, 2, 2, 3, 4, 3, 3, 3, 3, 3, 4, 2, 2, 4, 3, 3, 3, 3, 4, 2, 3, 4, 3, 3, 4, 3, 3, 2, 3, 5, 3, 2, 3, 3, 4, 1, 4, 4, 3, 3, 3, 3, 4, 5, 4, 2, 3, 2, 3, 4, 3, 3, 4, 2, 3, 2, 4, 4, 3, 1, 4, 4, 4, 4, 1, 4, 3, 4, 3, 4, 3, 3, 2, 3, 4, 4, 3, 3, 4, 4, 4, 3, 5, 3, 4, 3, 4, 3, 4, 3, 4, 5, 3, 4, 4, 4, 4, 3, 3, 3, 4, 4, 5, 4, 4, 4, 3, 5, 3, 5, 5, 4, 4, 2, 2, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 4, 3, 4, 1, 4, 3, 3, 4, 5, 4, 4, 4, 4, 2, 4, 2, 3, 3, 4, 3, 4, 3, 4, 4, 2, 3, 3, 4, 4, 4, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4, 2, 4, 3, 4, 3, 3, 3, 2, 4, 3, 4, 5, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 4, 2, 3, 4, 4, 4, 5, 5, 4, 4, 3, 4, 5, 2, 2, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, 4, 2, 3, 5, 3, 3, 3, 3, 5, 4, 3, 2, 3, 1, 3, 4, 4, 4, 5, 4, 4, 2, 4, 3, 3, 4, 3, 4, 2, 4, 4, 2, 3, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 4, 3, 4, 3, 3, 3, 3, 4, 2, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 4, 2, 4, 4, 3, 4, 3, 3, 4, 3, 3, 6, 3, 4, 3, 3, 3, 2, 4, 4, 3, 3, 3, 2, 4, 2, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 1, 3, 3, 3, 4, 3, 4, 3, 2, 3, 4, 3, 4, 3, 4, 3, 2, 4, 2, 3, 4, 3, 4, 4, 3, 3, 2, 4, 4, 4, 4, 4, 4, 3, 3, 4, 3, 4, 4, 3, 4, 5, 3, 4, 3, 3, 4, 4, 4, 3, 3, 5, 3, 3, 4, 3, 3, 3, 4, 2, 2, 4, 5, 4, 4, 2, 4, 3, 4, 4, 3, 4, 4, 3, 2, 4, 4, 3, 2, 3, 3, 3, 3, 4, 3, 4, 4, 2, 3, 4, 5, 2, 3, 2, 4, 4, 4, 5, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 4, 5, 3, 3, 3, 5, 4, 3, 3, 3, 2, 3, 3]\n",
      "predicted_scores: [4, 2, 4, 5, 5, 4, 5, 5, 4, 4, 4, 4, 5, 4, 3, 4, 5, 4, 3, 4, 3, 4, 3, 5, 5, 5, 3, 3, 4, 4, 4, 4, 3, 4, 4, 5, 4, 4, 4, 5, 5, 4, 6, 5, 4, 4, 5, 4, 5, 5, 5, 3, 4, 3, 5, 5, 6, 3, 4, 5, 3, 3, 5, 5, 4, 4, 4, 4, 3, 3, 4, 4, 4, 5, 4, 4, 5, 3, 5, 2, 4, 4, 5, 4, 4, 4, 4, 3, 2, 5, 3, 5, 4, 4, 5, 7, 5, 6, 4, 5, 4, 4, 5, 4, 4, 4, 5, 3, 3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 2, 4, 4, 5, 4, 3, 4, 4, 6, 4, 6, 5, 4, 5, 3, 4, 4, 4, 5, 4, 4, 4, 3, 5, 5, 5, 5, 4, 4, 5, 6, 5, 4, 6, 5, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 2, 3, 4, 5, 4, 4, 5, 3, 3, 4, 5, 3, 3, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 6, 5, 5, 4, 5, 5, 2, 4, 4, 6, 6, 3, 4, 4, 4, 4, 4, 4, 5, 4, 5, 3, 4, 4, 4, 5, 3, 3, 4, 5, 4, 5, 4, 5, 4, 6, 4, 5, 3, 5, 4, 5, 4, 6, 3, 5, 6, 4, 6, 4, 5, 3, 4, 5, 3, 4, 5, 5, 4, 2, 5, 4, 4, 4, 5, 5, 3, 4, 5, 4, 2, 4, 5, 3, 3, 3, 5, 4, 6, 3, 4, 5, 5, 5, 5, 4, 3, 5, 3, 5, 5, 4, 5, 3, 5, 6, 4, 5, 4, 4, 5, 3, 5, 4, 5, 4, 6, 5, 5, 3, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 6, 6, 5, 4, 4, 4, 4, 6, 5, 5, 4, 5, 3, 4, 4, 5, 4, 4, 4, 6, 5, 5, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 3, 5, 4, 4, 5, 3, 4, 6, 3, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 6, 6, 4, 3, 4, 4, 4, 4, 4, 3, 4, 3, 5, 4, 5, 4, 3, 4, 5, 4, 3, 4, 4, 5, 6, 5, 3, 5, 4, 4, 4, 4, 4, 4, 4, 3, 5, 4, 4, 4, 5, 5, 5, 5, 4, 3, 4, 4, 6, 3, 3, 5, 3, 4, 6, 5, 5, 5, 4, 5, 5, 4, 6, 5, 5, 4, 5, 4, 4, 5, 4, 5, 3, 4, 5, 6, 4, 5, 5, 3, 3, 5, 4, 3, 4, 6, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 5, 4, 3, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 3, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 5, 5, 5, 5, 4, 3, 2, 5, 3, 5, 4, 4, 4, 3, 4, 4, 4, 3, 5, 2, 5, 4, 4, 4, 3, 5, 3, 4, 4, 4, 4, 4, 4, 2, 5, 4, 3, 4, 3, 4, 5, 4, 6, 3, 4, 5, 4, 4, 4, 5, 3, 4, 5, 5, 4, 4, 5, 4, 5, 3, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 5, 4, 6, 5, 5, 5, 3, 5, 5, 6, 4, 4, 4, 5, 5, 4, 4, 4, 5, 2, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 3, 4, 4, 6, 4, 4, 4, 5, 3, 4, 3, 3, 4, 3, 3, 3, 3, 6, 5, 5, 4, 5, 6, 5, 5, 3, 4, 4, 4, 5, 3, 3, 3, 5, 4, 6, 4, 5, 4, 3, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 4, 3, 4, 4, 5, 4, 5, 4, 4, 4, 3, 4, 3, 3, 5, 5, 4, 3, 5, 3, 4, 3, 4, 3, 4, 5, 5, 5, 5, 5, 5, 6, 4, 4, 3, 5, 5, 3, 4, 5, 5, 3, 4, 4, 4, 4, 5, 4, 2, 4, 4, 3, 4, 4, 5, 5, 4, 4, 4, 4, 5, 4, 3, 3, 4, 5, 4, 5, 3, 5, 4, 4, 5, 4, 4, 5, 5, 3, 5, 4, 3, 3, 4, 3, 4, 5, 5, 4, 4, 4, 4, 5, 3, 4, 5, 4, 4, 5, 4, 5, 5, 6, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 2, 4, 5, 5, 4, 3, 5, 2, 5, 4, 3, 3, 5, 4, 4, 4, 5, 6, 4, 4, 3, 5, 5, 4, 4, 3, 4, 5, 3, 5, 4, 5, 4, 4, 6, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 3, 5, 4, 4, 5, 6, 3, 4, 4, 5, 4, 4, 4, 4, 6, 4, 5, 4, 4, 6, 3, 4, 3, 5, 3, 5, 3, 4, 4, 4, 6, 4, 4, 4, 4, 4, 4, 4, 3, 4, 5, 4, 4, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 6, 3, 4, 5, 5, 3, 5, 5, 5, 5, 3, 4, 4, 4, 4, 3, 4, 5, 4, 4, 4, 4, 6, 4, 5, 4, 3, 4, 5, 5, 4, 5, 4, 6, 3, 6, 5, 5, 4, 2, 3, 5, 3, 3, 4, 4, 5, 4, 5, 5, 5, 4, 4, 6, 5, 4, 5, 4, 5, 4, 4, 4, 5, 2, 5, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 6, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 5, 4, 5, 4, 5, 4, 4, 5, 4, 5, 4, 3, 4, 4, 4, 4, 4, 4, 4, 5, 4, 6, 3, 5, 4, 5, 6, 4, 5, 5, 4, 6, 5, 5, 4, 5, 3, 3, 4, 5, 5, 4, 3, 3, 1, 4, 4, 5, 5, 4, 3, 6, 4, 6, 5, 4, 5, 3, 4, 4, 5, 4, 5, 4, 5, 4, 5, 4, 4, 5, 3, 4, 4, 5, 3, 5, 4, 4, 2, 3, 6, 4, 4, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 4, 2, 5, 6, 3, 5, 4, 4, 3, 4, 5, 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 4, 6, 3, 4, 5, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 3, 5, 4, 5, 4, 4, 4, 4, 6, 5, 4, 6, 2, 4, 4, 3, 4, 5, 4, 3, 4, 4, 3, 5, 3, 5, 5, 5, 5, 5, 6, 5, 4, 5, 6, 4, 3, 6, 5, 5, 3, 6, 4, 4, 5, 5, 4, 4, 5, 4, 3, 3, 3, 4, 4, 3, 3, 4, 5, 3, 5, 4, 6, 3, 4, 6, 5, 3, 4, 5, 3, 5, 5, 4, 5, 4, 6, 6, 4, 4, 4, 4, 4, 6, 4, 6, 5, 3, 4, 4, 5, 2, 5, 3, 5, 5, 5, 4, 4, 5, 3, 5, 5, 5, 3, 4, 3, 4, 7, 5, 3, 3, 4, 4, 5, 3, 3, 8, 4, 6, 5, 3, 5, 4, 4, 5, 6, 4, 5, 5, 5, 3, 6, 4, 4, 4, 3, 3, 4, 4, 2, 6, 4, 3, 5, 3, 5, 4, 4, 4, 6, 4, 5, 3, 6, 4, 4, 5, 4, 4, 5, 6, 4, 4, 3, 5, 4, 5, 4, 3, 5, 4, 5, 3, 4, 5, 3, 5, 4, 4, 4, 5, 4, 5, 2, 4, 5, 5, 4, 4, 6, 3, 3, 6, 4, 4, 4, 4, 4, 4, 6, 5, 4, 5, 4, 5, 5, 4, 5, 2, 3, 3, 5, 4, 4, 4, 4, 3, 5, 2, 3, 5, 4, 4, 3, 5, 4, 3, 4, 5, 4, 4, 5, 3, 4, 3, 4, 5, 4, 3, 4, 4, 4, 2, 5, 4, 3, 3, 4, 4, 5, 6, 4, 2, 4, 3, 3, 4, 5, 3, 5, 3, 4, 3, 5, 5, 3, 3, 4, 5, 4, 4, 2, 4, 4, 5, 5, 5, 4, 5, 2, 3, 5, 5, 4, 3, 5, 4, 4, 4, 6, 5, 4, 4, 4, 4, 5, 4, 4, 6, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 7, 4, 6, 6, 4, 4, 3, 2, 5, 4, 4, 5, 5, 6, 3, 4, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 4, 3, 5, 3, 5, 6, 5, 3, 5, 4, 4, 2, 5, 4, 4, 5, 5, 5, 4, 6, 5, 3, 4, 3, 4, 4, 5, 4, 5, 4, 4, 5, 2, 4, 4, 5, 4, 4, 5, 3, 5, 5, 5, 5, 4, 4, 4, 4, 5, 3, 5, 4, 4, 4, 5, 5, 5, 3, 5, 4, 6, 4, 3, 4, 2, 4, 2, 4, 5, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 3, 5, 4, 4, 4, 6, 4, 4, 3, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 6, 5, 5, 4, 4, 5, 2, 3, 4, 3, 4, 4, 5, 4, 3, 4, 4, 4, 5, 2, 4, 6, 4, 5, 4, 3, 5, 4, 4, 4, 4, 2, 4, 5, 4, 6, 6, 4, 5, 2, 4, 3, 4, 5, 5, 5, 3, 6, 4, 3, 5, 4, 4, 5, 3, 3, 4, 3, 3, 5, 6, 6, 3, 5, 4, 4, 3, 3, 5, 3, 5, 6, 5, 4, 4, 5, 5, 4, 4, 4, 4, 3, 4, 6, 4, 4, 4, 3, 6, 4, 4, 6, 5, 6, 4, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 3, 5, 5, 5, 4, 4, 5, 4, 4, 4, 4, 2, 3, 4, 4, 5, 4, 6, 3, 3, 5, 5, 3, 5, 4, 5, 3, 4, 5, 3, 3, 5, 4, 4, 4, 4, 4, 3, 4, 6, 5, 4, 4, 4, 4, 4, 5, 3, 4, 6, 4, 5, 6, 4, 6, 3, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 5, 2, 3, 5, 7, 5, 5, 3, 6, 4, 5, 5, 4, 4, 5, 4, 2, 4, 4, 4, 3, 3, 3, 4, 4, 5, 4, 5, 5, 3, 4, 4, 5, 4, 3, 3, 6, 4, 5, 6, 5, 5, 4, 4, 3, 5, 4, 4, 3, 5, 5, 6, 4, 5, 6, 5, 4, 3, 3, 4, 3, 4, 5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_and_save_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain_and_save_models\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining model for Prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     best_params, best_QWK \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     layers_size, hidden_size, lr, batch_size \u001b[38;5;241m=\u001b[39m best_params\n\u001b[0;32m     15\u001b[0m    \u001b[38;5;66;03m## train model based on best hyperparameters\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mfind_best_hyperparameters\u001b[1;34m(prompt_num, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m learning_rate:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Testing: Layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHidden size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Learning rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     QWK \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m QWK \u001b[38;5;241m>\u001b[39m best_QWK:\n\u001b[0;32m     16\u001b[0m         best_QWK \u001b[38;5;241m=\u001b[39m QWK\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(prompt_num, layer_size, hidden_size, lr, device, batch_size, num_epochs)\u001b[0m\n\u001b[0;32m     33\u001b[0m     train_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(train_scores, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(train_features, train_scores)\n\u001b[1;32m---> 38\u001b[0m     train_qwk,model\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrange_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     avg_QWKs\u001b[38;5;241m.\u001b[39mappend(train_qwk )\n\u001b[0;32m     40\u001b[0m final_avg_QWK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(avg_QWKs) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(avg_QWKs)\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss, optimizer, train_set, validation_set, device, range_score, batch_size, num_epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\n\u001b[0;32m     20\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_pred, scores)\n\u001b[1;32m---> 21\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.026377689563606106\n",
      "Epoch 2 loss: 0.02124779043692186\n",
      "Epoch 3 loss: 0.020774515122120668\n",
      "Epoch 4 loss: 0.020323486224034487\n",
      "Epoch 5 loss: 0.02031435543136113\n",
      "Epoch 6 loss: 0.02011306774689105\n",
      "Epoch 7 loss: 0.019970201749171437\n",
      "Epoch 8 loss: 0.019756000666777487\n",
      "Epoch 9 loss: 0.01970939425992436\n",
      "Epoch 10 loss: 0.019839951791915396\n",
      "Epoch 11 loss: 0.01958637582300387\n",
      "Epoch 12 loss: 0.01963564121720907\n",
      "Epoch 13 loss: 0.019618336626383938\n",
      "Epoch 14 loss: 0.019421054552950574\n",
      "Epoch 15 loss: 0.01951118941021493\n"
     ]
    }
   ],
   "source": [
    "## train the deployed model\n",
    "import torch\n",
    "def train_Deployed():\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = HolisticFFN(86, 1, 32, 1)\n",
    "            model.to(device)\n",
    "            loss = nn.MSELoss()\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.1)\n",
    "            train_set = get_trainSet(None, device)\n",
    "\n",
    "            qwk, trained_model = train_model(model, loss, optimizer, train_set, None, device,None, 8, num_epochs=15)\n",
    "            scrtipted_model = torch.jit.script(trained_model)\n",
    "            scrtipted_model.save(\"model-A-deploy.pt\")\n",
    "\n",
    "    \n",
    "\n",
    "train_Deployed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
