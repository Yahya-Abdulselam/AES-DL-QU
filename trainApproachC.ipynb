{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils as gu\n",
    "data = gu.read_data(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_score(score, lower_bound, upper_bound):\n",
    "    range = upper_bound - lower_bound\n",
    "    return (score - lower_bound) / range # we divide the difference from the minimum value by the range to get a value between 0 and 1\n",
    "def unscale_score(scaled_score, lower_bound, upper_bound):\n",
    "    range=upper_bound-lower_bound\n",
    "    return scaled_score*range+lower_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def get_bert_embedding_essay(essay):\n",
    "    embedding=tokenizer(essay,add_special_tokens=True,padding='max_length',truncation=True,return_tensors='pt')\n",
    "    return embedding['input_ids'].view(-1),embedding['attention_mask'].view(-1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dicts=[] # we will store the data for each prompt in a list of dictionaries, one dictionary \n",
    "#  one dictionary  will have prompt_id and a list of essays for that prompt with their features and scaled holistic scores\n",
    "for prompt_id in range(1,9):\n",
    "    essays_for_prompt=[]\n",
    "    for line_num in range(len(data['essay_ids'])):\n",
    "        \n",
    "        if data['prompt_ids'][line_num]==prompt_id:\n",
    "            range_score=gu.SCORE_RANGES[prompt_id]['holistic']\n",
    "            input_ids,attention_mask=get_bert_embedding_essay(data[\"essay_text\"][line_num])\n",
    "            essays_for_prompt.append({'essay_text':data[\"essay_text\"][line_num],'holistic':scale_score(data['holistic'][line_num],range_score[0],range_score[1]),'features':data['features'][line_num],'input_ids':input_ids,'attention_mask':attention_mask})\n",
    "\n",
    "    prompts_dicts.append({'prompt_id':prompt_id,'essays':essays_for_prompt})\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "def get_layers(input_size, layers_size, hidden_size, output_size):\n",
    "    layers = []\n",
    "    if layers_size == 0:\n",
    "        # No hidden layers; direct input to output\n",
    "        layers.append(nn.Linear(input_size, output_size))\n",
    "    else:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "          \n",
    "            layers.append(nn.Linear(hidden_size, output_size))\n",
    "    return layers\n",
    "\n",
    "class BERT_approachC(nn.Module):\n",
    "    def __init__(self,bert_model,layers_size,num_hidden_units,input_size=86,output_size=1):\n",
    "        super(BERT_approachC,self).__init__()\n",
    "        self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.additional_layer=nn.Sequential(*get_layers(self.bert.config.hidden_size+input_size,layers_size,num_hidden_units,output_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.apply(self.weights_HE_init)\n",
    "    def forward(self,input_ids,attention_mask,features):\n",
    "        output=self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        cls=output['pooler_output']\n",
    "    \n",
    "        cls_features=torch.cat((cls,features),dim=1)\n",
    "  \n",
    "        return self.additional_layer(cls_features)\n",
    "    def weights_HE_init(self,layer_in):\n",
    "     if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer_in.weight)\n",
    "        layer_in.bias.data.fill_(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def train_model(model, loss, optimizer, train_set, validation_set, device, range_score, batch_size=4, num_epochs=5):\n",
    "    last_qwk = 0\n",
    "\n",
    "    model.to(device)# move the model to the gpu if available in the machine\n",
    "    train_set = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    validation_setLoader = DataLoader(validation_set)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs ):\n",
    "        print(\"*\"*10)\n",
    "        model.train() \n",
    "        total_loss = 0\n",
    "        for features, scores,input_ids,attention_mask in train_set:\n",
    "            features, scores = features.to(device), scores.to(device)\n",
    "            input_ids,attention_mask=input_ids.to(device),attention_mask.to(device)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()# zero the gradients for the previous iteration\n",
    "\n",
    "\n",
    "            y_pred = model(input_ids,attention_mask,features).view(-1)# view(-1) was used to flatten outputo ensure its same as true scores\n",
    "            scores = scores\n",
    "            l = loss(y_pred, scores)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += l.item()\n",
    "        print(f\"Epoch {epoch + 1} loss: {total_loss/len(train_set)}\")\n",
    "        if validation_set is None: # in some cases we may not have validation set like when training the deployed model\n",
    "            continue\n",
    "        model.eval()# set the model to evaluation mode\n",
    "        predicted_scores, validation_scores = [], [] # to store the predicted and true scores\n",
    "        total_loss = 0\n",
    "        with torch.no_grad(): # we don't need to calculate gradients for validation set\n",
    "            \n",
    "            for features, scores,input_ids,attention_mask in validation_setLoader:\n",
    "                features, scores = features.to(device), scores.to(device)\n",
    "                input_ids,attention_mask=input_ids.to(device),attention_mask.to(device)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "                y_pred = model(input_ids,attention_mask,features).view(-1)\n",
    "                l=loss(y_pred, scores)\n",
    "                total_loss += l.item()\n",
    "       \n",
    "             \n",
    "                predicted_scores.append(y_pred.item())\n",
    "                validation_scores.append(scores.item())\n",
    "\n",
    "          # qwk only works with integers\n",
    "            validation_scores = [int(round(unscale_score(score, range_score[0], range_score[1]))) for score in validation_scores] #\n",
    "            predicted_scores = [int(round(unscale_score(pred, range_score[0], range_score[1]))) for pred in predicted_scores]\n",
    "      \n",
    "            last_qwk = gu.quadratic_weighted_kappa(validation_scores, predicted_scores)\n",
    "            print(f\"{epoch + 1} qwk: {last_qwk}  \")\n",
    "\n",
    "    \n",
    "    return last_qwk,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertModel\n",
    "def cross_validation( layer_size,hidden_size, device,batch_size=4, num_epochs=5,prompt_num=1):\n",
    "\n",
    "    avg_QWKs = []\n",
    "    # each prompt will be used as the validation set once\n",
    "    for fold in range(8): \n",
    "        print('fold: ',fold)\n",
    "        if prompt_num == fold+1: # we don't want to train on the validation set\n",
    "            continue\n",
    "        range_score=gu.SCORE_RANGES[fold+1]['holistic']\n",
    "        # normal lists are not compatible with pytorch, so we convert them to  numpy arrays and then to tensors\n",
    "        features = torch.tensor(np.array([essay['features'] for essay in prompts_dicts[fold]['essays']], dtype=np.float32)).to(device)\n",
    "        scores = torch.tensor(np.array([essay['holistic'] for essay in prompts_dicts[fold]['essays']], dtype=np.float32)).to(device)\n",
    "        input_ids = torch.tensor(np.array([essay['input_ids'] for essay in prompts_dicts[fold]['essays']], dtype=np.int64)).to(device)\n",
    "        attention_mask = torch.tensor(np.array([essay['attention_mask'] for essay in prompts_dicts[fold]['essays']], dtype=np.int64)).to(device)\n",
    "        validation_dataset=TensorDataset(features, scores,input_ids,attention_mask)\n",
    "\n",
    "        train_features = []\n",
    "        train_scores = []\n",
    "        train_input_ids = []\n",
    "        train_attention_mask = []\n",
    "        for prompt_index in range(8):\n",
    "          # we don't want to train on the validation set nor the test set\n",
    "          if prompt_num == fold+1 or prompt_index==fold:\n",
    "            continue\n",
    "          train_features.extend([essay['features'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "          train_scores.extend([essay['holistic'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "          train_input_ids.extend([essay['input_ids'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "          train_attention_mask.extend([essay['attention_mask'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "\n",
    "        model=BERT_approachC('bert-base-uncased',layer_size,hidden_size,86,1)\n",
    "\n",
    "        model.to(device)\n",
    "        loss=nn.MSELoss()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), 0.005,betas=(0.9, 0.999),  weight_decay=0.1)\n",
    "       # normal lists are not compatible with pytorch, so we convert them to  numpy arrays and then to tensors\n",
    "        train_features = torch.tensor(np.array(train_features, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "        train_scores = torch.tensor(np.array(train_scores, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "        train_attention_mask = torch.tensor(np.array(train_attention_mask, dtype=np.int64), dtype=torch.int64).to(device)\n",
    "        train_input_ids = torch.tensor(np.array(train_input_ids, dtype=np.int64), dtype=torch.int64).to(device)\n",
    "   \n",
    "        train_dataset = TensorDataset(train_features, train_scores,train_input_ids,train_attention_mask)\n",
    "       \n",
    "     \n",
    "        train_qwk,model=train_model(model, loss, optimizer,train_dataset, validation_dataset,device,range_score,batch_size=batch_size,num_epochs=num_epochs)\n",
    "        avg_QWKs.append(train_qwk )\n",
    "    final_avg_QWK = sum(avg_QWKs) / len(avg_QWKs)\n",
    "    print(f\"Average QWK over all folds: {final_avg_QWK}\")\n",
    "\n",
    "    return final_avg_QWK\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_options=[{'hidden':0},{'hidden':4},{'hidden':8}]\n",
    "\n",
    "def find_best_hyperparameters(device,prompt_num=1):\n",
    "   \n",
    "    best_QWK = 0\n",
    "    best_params = None\n",
    "    latest_params = None\n",
    "  \n",
    "    for option in hyper_options:\n",
    "        print(f\"Testing option: {option}\")\n",
    "        if option['hidden'] == 0:\n",
    "           QWK=cross_validation(0, 0, device, 32,5,1)\n",
    "           latest_params = (0, 0)\n",
    "              \n",
    "        elif(option['hidden'] == 4):\n",
    "            QWK=cross_validation(1, 4, device, 32,5,1)\n",
    "            latest_params = (1, 4)\n",
    "        elif(option['hidden'] == 8):\n",
    "            QWK=cross_validation(1, 8, device, 32,5,1)\n",
    "            latest_params = (1, 8)\n",
    "            \n",
    "        if QWK > best_QWK:\n",
    "                    best_QWK = QWK\n",
    "                    best_params =latest_params\n",
    "                    print(f\"New best QWK: {best_QWK:.4f} with params: {best_params}\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return best_params, best_QWK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_validation_set():\n",
    "    valid_scores=torch.tensor(np.array([essay['holistic'] for essay in prompts_dicts[0]['essays']], dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    valid_features=torch.tensor(np.array([essay['features'] for essay in prompts_dicts[0]['essays']], dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    valid_input_ids=torch.tensor(np.array([essay['input_ids'] for essay in prompts_dicts[0]['essays']], dtype=np.int64), dtype=torch.int64).to(device)\n",
    "    valid_attention_mask=torch.tensor(np.array([essay['attention_mask'] for essay in prompts_dicts[0]['essays']], dtype=np.int64), dtype=torch.int64).to(device)\n",
    "    val_set = TensorDataset(valid_features, valid_scores,valid_input_ids,valid_attention_mask)\n",
    "    return val_set\n",
    "def get_train_set():\n",
    "    train_features = []\n",
    "    train_scores = []\n",
    "    train_input_ids = []\n",
    "    train_attention_mask = []\n",
    "    for prompt_index in range(8):\n",
    "        if prompt_index==0:\n",
    "            continue\n",
    "        train_features.extend([essay['features'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        train_scores.extend([essay['holistic'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        train_input_ids.extend([essay['input_ids'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        train_attention_mask.extend([essay['attention_mask'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "    train_features = torch.tensor(np.array(train_features, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    train_scores = torch.tensor(np.array(train_scores, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    train_attention_mask = torch.tensor(np.array(train_attention_mask, dtype=np.int64), dtype=torch.int64).to(device)\n",
    "    train_input_ids = torch.tensor(np.array(train_input_ids, dtype=np.int64), dtype=torch.int64).to(device)\n",
    "    train_set = TensorDataset(train_features, train_scores,train_input_ids,train_attention_mask)\n",
    "    return train_set\n",
    "def train_and_save_models():\n",
    "   \n",
    "# gpu if available\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "    \n",
    "    best_params, best_QWK = find_best_hyperparameters(device, 1)\n",
    "    layers_size, hidden_size= best_params\n",
    "    \n",
    "    ## train model based on best hyperparameters\n",
    "    model = BERT_approachC('bert-base-uncased', layers_size, hidden_size, 86,1)\n",
    "    model.to(device)\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, \n",
    "                                betas=(0.9, 0.999), weight_decay=0.1)\n",
    "    val_set=get_validation_set()\n",
    "    train_set=get_train_set()\n",
    "    \n",
    "\n",
    "    qwk, trained_model = train_model(model, loss, optimizer, train_set, \n",
    "                                    val_set, device, gu.SCORE_RANGES[1]['holistic'], \n",
    "                                    32, num_epochs=5)\n",
    "    print('params:',best_params)\n",
    "    print('best qwk:',best_QWK)\n",
    "    torch.save(trained_model.state_dict(), f\"model_C_1Final++.pt\")\n",
    "    \n",
    "    \n",
    "    results = {\n",
    "        'prompt num': 1,\n",
    "        'layers_size': layers_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'batch_size': 32,\n",
    "        'qwk': qwk\n",
    "    }\n",
    "    \n",
    "\n",
    "    print(results)\n",
    "    \n",
    "\n",
    "    print(f\"\\nBest Overall QWK: {best_QWK:.4f}\")\n",
    "    return best_QWK, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_C_1():\n",
    "    model = BERT_approachC('bert-base-uncased', 0, 0, 86, 1)\n",
    "    model.load_state_dict(torch.load(\"model_C_1.pt\"))\n",
    "    return model\n",
    "def train_Deployed_model(layer_size,hidden_size,device):\n",
    "    model = BERT_approachC('bert-base-uncased', layer_size, hidden_size, 86, 1)\n",
    "    model.to(device)\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, \n",
    "                                betas=(0.9, 0.999), weight_decay=0.1)\n",
    "  # train on all data 8 prompts\n",
    "    features = []\n",
    "    scores = []\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for prompt_index in range(8):\n",
    "        features.extend([essay['features'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        scores.extend([essay['holistic'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        input_ids.extend([essay['input_ids'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "        attention_mask.extend([essay['attention_mask'] for essay in prompts_dicts[prompt_index]['essays']])\n",
    "    features = torch.tensor(np.array(features, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    scores = torch.tensor(np.array(scores, dtype=np.float32), dtype=torch.float32).to(device)\n",
    "    attention_mask = torch.tensor(np.array(attention_mask, dtype=np.int64), dtype=torch.int64).to(device)\n",
    "    input_ids = torch.tensor(np.array(input_ids, dtype=np.int64), dtype=torch.int64).to(device)\n",
    "    dataset = TensorDataset(features, scores,input_ids,attention_mask)\n",
    "    qwk, trained_model = train_model(model, loss, optimizer, dataset, None, device,None, 32, num_epochs=5)\n",
    "    torch.save(trained_model.state_dict(), f\"model_C_1_deployed.pt\")\n",
    "    return qwk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_Deployed_model(0,0,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
